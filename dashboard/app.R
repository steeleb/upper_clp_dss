#Protoype dashboard

library(shinydashboard)
library(shinyWidgets)
library(shinycssloaders)
library(shiny)
library(dplyr)
library(lubridate)
library(readr)
library(yaml)
library(ggplot2)
library(tidyverse)
library(rsconnect)
library(bslib)
library(plotly)
library(cdssr)
library(dataRetrieval)
library(patchwork)
library(shinyTime)
library(DT)
library(leaflet)
library(htmltools)
library(arrow)
options(shiny.maxRequestSize = 10000 * 1024^2)

site_table <- tibble(site_code = c("sfm", "chd", "pfal", "pbd", "pbr_fc", "pman_fc"),
                     site_name = c("South Fork CLP", "Chambers Lake Outflow", "CLP @ Poudre Falls", "Canyon Mouth", "CLP @ Indian Meadows", "CLP @ Manners Bridge"),
                     color = c("#002EA3", "#E70870", "#256BF5", "#1E4D2B", "#56104E", "#FFCA3A"))
# Source setup and functions (assuming these exist)
source("src/setup_libraries.R")

# Define UI
ui <- dashboardPage(
  dashboardHeader(title = "Water Quality Monitoring Dashboard"),

  dashboardSidebar(
    sidebarMenu(
      menuItem("Live WQ Data", tabName = "sensor_data", icon = icon("chart-line")),
      #Placeholder for future functions
      #menuItem("WQ Forecast", tabName = "forecast", icon = icon("bolt"))
      menuItem("CLP Basin Conditions", tabName = "flow_data", icon = icon("droplet")),
      menuItem("Site Map", tabName = "map", icon = icon("map"))
      #Placeholder for future functions
      #menuItem("Settings", tabName = "settings", icon = icon("cog"))
    )
  ),

  dashboardBody(
    tags$head(
      tags$style(HTML("
        .content-wrapper, .right-side {
          background-color: #f4f4f4;
        }
      "))
    ),

    tabItems(
      # Data Overview Tab
      tabItem(tabName = "sensor_data",
              fluidRow(
                box(
                  title = "Data Controls", status = "primary", solidHeader = TRUE, width = 12,
                  fluidRow(
                    column(4,
                           dateRangeInput("date_range",
                                          label = "Select Date Range:",
                                          start = Sys.Date() - 3,
                                          end = Sys.Date(),
                                          max = Sys.Date()
                           ),
                           pickerInput("sites_select",
                                       label = "Select Sites:",
                                       choices = c("South Fork CLP", "Chambers Lake Outflow", "CLP @ Poudre Falls", "Canyon Mouth", "CLP @ Indian Meadows", "CLP @ Manners Bridge"),
                                       selected = c("South Fork CLP", "Chambers Lake Outflow", "CLP @ Poudre Falls", "Canyon Mouth", "CLP @ Indian Meadows", "CLP @ Manners Bridge"),
                                       multiple = TRUE,
                                       options = pickerOptions(
                                         actionsBox = TRUE,
                                         selectAllText = "Select All",
                                         deselectAllText = "Deselect All"
                                       )
                           )
                    ),
                    column(4,
                           pickerInput("parameters_select",
                                       label = "Select Parameters:",
                                       choices = c("Temperature", "Turbidity", "pH", "DO",
                                                   "Specific Conductivity", "Chl-a Fluorescence", "FDOM Fluorescence", "Depth"),
                                       selected = c("Temperature", "Turbidity", "pH", "Specific Conductivity"),
                                       multiple = TRUE,
                                       options = pickerOptions(
                                         actionsBox = TRUE,
                                         selectAllText = "Select All",
                                         deselectAllText = "Deselect All"
                                       )
                           ),
                           actionButton("load_data", "Load Selected Dataset")
                    ),
                    column(4,
                           h4("Log Scale Controls:"),
                           br(),
                           uiOutput("log_controls"),
                           br(),
                           downloadButton("download_data", "Download Data",
                                          class = "btn-success")
                    )
                  )
                )
              ),

              fluidRow(
                box(
                  title = "Time Series Plots", status = "primary", solidHeader = TRUE, width = 12,
                  uiOutput("dynamic_plots") %>% withSpinner()
                )
              ),
              fluidRow(
                box(
                  title = "Modelled Time Series plots", status = "primary", solidHeader = TRUE, width = 12,
                  #uiOutput("dynamic_plots") %>% withSpinner()
                )
              )


      ),
      # Site Map Tab
      tabItem(tabName = "map",
              fluidRow(
                box(
                  title = "Site Locations", status = "primary", solidHeader = TRUE, width = 12,
                  leafletOutput("site_map", height = "600px") %>% withSpinner()
                )
              )
      ),
      # Flow Data Tab
      tabItem(tabName = "flow_data",
              layout_columns(
                col_widths = c(7, 5),
                # 3-day Q plot card
                card(
                  card_header("Current flows (7 days) for key CLP sites"),
                  card_body(
                    plotlyOutput("three_day_q_plot")
                  )
                ),
                # Conditional SNOTEL card that will only show between October and July
                uiOutput("snotel_card")
              ),
              # Main Map Card
              card(
                card_header("Tracking Flow rates through the CLP network"),
                card_body(
                  leafletOutput("map", height = 600)
                ),
                card_footer(
                  "Data retrieved from USGS/CDWR using the cdssr package. Colors indicate flow trend in last 24 hours: red (decreasing), green (increasing), grey (no data)."
                )
              )
      )

    )
  )
)

# Define Server
server <- function(input, output, session) {



  # Reactive values for storing data
  values <- reactiveValues(
    all_data = NULL,
    site_locations = NULL,
    flow_sites = NULL,
    last_refresh = Sys.time()
  )
  loaded_data <- reactiveVal()



  # Render the site map with leaflet
  observeEvent(input$sites_select, {

sites_sel <- filter(site_table, site_name %in% input$sites_select )%>%
  pull(site_code)


    # Sample site locations
    values$site_locations <- read_csv(here("data", "metadata", "sonde_location_metadata.csv"), show_col_types = F) %>%
      separate(col = "lat_long", into = c("lat", "lon"), sep = ",", convert = TRUE) %>%
      st_as_sf(coords = c("lon", "lat"), crs = 4326) %>%
      mutate(site = tolower(Site),
             site = ifelse(site %in% c("pman", "pbr"), paste0(site, "_fc"), site))%>%
      filter(site %in% sites_sel)
  })

  # Initialize data on load data click

  loaded_data <- eventReactive(input$load_data, {
    req(input$date_range, input$sites_select, input$parameters_select)


    sites_sel <- filter(site_table, site_name %in% input$sites_select )%>%
      pull(site_code)

    # Validate inputs
    withProgress(message = "Retrieving CLP WQ Data...", {

    start_DT <- as.POSIXct(paste0(input$date_range[1], " 00:01"), tz = "America/Denver")
    end_DT <- as.POSIXct(paste0(input$date_range[2], " 23:55"), tz = "America/Denver")

      #### WET API Pull ####
    #check to see if we need to pull WET data
    if(any(c("sfm", "chd", "pfal") %in% sites_sel)){

      invalid_values <- c(-9999, 638.30, -99.99)
      # Define sites to pull data for
      wet_sites <- c("sfm", "chd", "pfal")
      #grab the sites from sites_sel
      sites <- sites_sel[sites_sel %in% wet_sites]


      incProgress(0.2, detail = "Connecting to WET API...")

      source("src/pull_wet_api.R")

      wet_data <- map(sites,
                      ~pull_wet_api(
                        target_site = .x,
                        start_datetime = start_DT,
                        end_datetime = end_DT,
                        data_type = "all",
                        time_window = "all"
                      )) %>%
        rbindlist() %>%
        #remove invalid values or NAs
        filter(value %nin% invalid_values, !is.na(value)) %>%
        split(f = list(.$site, .$parameter), sep = "-")

    }else{
      #return blank list
      wet_data <- list()
    }

## HydroVu Pull

      # check to see if we need to pull from PBD
      if("pbd" %in% sites_sel){
        sites <- c("pbd")

        # source scripts to pull Hydro Vu
        walk(list.files('src/fcw_QAQC_func/', pattern = "*.R", full.names = TRUE, recursive = TRUE), source)

        incProgress(0.4, detail = "Connecting to Hydro Vu API...")
        # Set up parallel processing
        num_workers <- min(availableCores() - 1, 4) # Use at most 4 workers
        plan(multisession, workers = num_workers)
        furrr_options(
          globals = TRUE,
          packages = c("arrow", "data.table", "httr2", "tidyverse", "lubridate", "zoo",
                       "padr", "stats", "RcppRoll", "yaml", "here", "fcw.qaqc")
        )

        # suppress scientific notation to ensure consistent formatting
        options(scipen = 999)

        # Establishing staging directory - Replacing with temp_dir()
        staging_directory = tempdir()

        # Read in credentials
        hv_creds <- read_yaml(here("data", "upper_clp_dss", "creds", "HydroVuCreds.yml"))
        hv_token <- hv_auth(client_id = as.character(hv_creds["client"]),
                            client_secret = as.character(hv_creds["secret"]))

        # Pulling in the data from hydrovu
        # Making the list of sites that we need
        hv_sites <- hv_locations_all(hv_token) %>%
          filter(!grepl("vulink", name, ignore.case = TRUE)) %>%
          #sondes with 2024 in the name can be avoided to speed up the live data pull
          #these should be included in the historical data pull
          filter(!grepl("2024", name, ignore.case = TRUE))

        walk(sites,
             function(site) {
               message("Requesting HV data for: ", site)
               api_puller(
                 site = site,
                 network = "all",
                 start_dt = with_tz(start_DT, tzone = "UTC"), # api puller needs UTC dates
                 end_dt = with_tz(end_DT, tzone = "UTC"),
                 api_token = hv_token,
                 hv_sites_arg = hv_sites,
                 dump_dir = staging_directory
               )
             }
        )

        # read in data from staging directory
        hv_data <- list.files(staging_directory, full.names = TRUE, pattern = ".parquet") %>%
          map_dfr(function(file_path){
            site_df <- read_parquet(file_path, as_data_frame = TRUE)
            return(site_df)
          }) %>%
          #doing some clean up
          select(-id) %>%
          mutate(units = as.character(units)) %>%
          #double check that Vulink data has been removed
          filter(!grepl("vulink", name, ignore.case = TRUE)) %>%
          mutate(
            DT = timestamp, #timestamp comes in UTC
            DT_round = round_date(DT, "15 minutes"), #rounding
            #DT_round_MT = with_tz(DT_round, tzone = "America/Denver"),
            DT_join = as.character(DT_round), #keeping in UTC but character form
            site = tolower(site)) %>%
          select(-name) %>%
          distinct(.keep_all = TRUE) %>%
          split(f = list(.$site, .$parameter), sep = "-") %>%
          keep(~nrow(.) > 0)
      }else{
        hv_data <- list()
      }

#Contrail pull

      #check to see if we need to access contrail
      if(any(c("pbr_fc", "pman_fc") %in% sites_sel)) {
        incProgress(0.7, detail = "Connecting to Contrail API...")
        source("src/pull_contrail_api.R")

        # Define sites to pull data for
        contrail_sites <- c("pbr_fc", "pman_fc")
        #grab the sites from sites_sel
        sites <- sites_sel[sites_sel %in% contrail_sites]

        trim_sites <- toupper(gsub("_fc", "", sites))

      # Read credentials
      creds <- yaml::read_yaml(here("creds","contrail_creds.yml")) %>%
        unlist()
      username <- as.character(creds["username"])
      password <- as.character(creds["password"])

      contrail_api_urls <- read_csv(here("data","upper_clp_dss", "creds", "contrail_device_urls.csv"), show_col_types = F)%>%
        filter(site_code %in% trim_sites)
      # Define the folder path where the CSV files are stored
      # Call the downloader function
      contrail_data <- pull_contrail_api(start_DT, end_DT, username, password, contrail_api_urls) %>%
        rbindlist() %>%
        split(f = list(.$site, .$parameter), sep = "-") %>%
        keep(~nrow(.) > 0)
      }else{
        contrail_data <- list()
      }

#### Data Aggregation  ####

      incProgress(0.9, detail = "Processing data...")
      # combine all data
      all_data_raw <- c(hv_data, wet_data, contrail_data)

      # remove stage data
      list_names <- names(all_data_raw)
      keep_indices <- !grepl("stage", list_names, ignore.case = TRUE)
      all_data_raw <- all_data_raw[keep_indices]

      # Tidy all the raw files
      tidy_data <- all_data_raw %>%
        map(~tidy_api_data(api_data = .)) %>%  # the summarize interval default is 15 minutes
        keep(~!is.null(.))

      #add field notes
      # Pulling in the data from mWater (where we record our field notes)
      mWater_creds <- read_yaml(here("creds", "mWaterCreds.yml"))
      mWater_data <- load_mWater(creds = mWater_creds)
      all_field_notes <- grab_mWater_sensor_notes(mWater_api_data = mWater_data) %>%
        #notes come in as MST, converting to UTC
        mutate(DT_round = with_tz(DT_round, tzone = "UTC"),
               last_site_visit = with_tz(last_site_visit, tzone = "UTC"),
               DT_join = as.character(DT_round))

      sensor_malfunction_notes <- grab_mWater_malfunction_notes(mWater_api_data = mWater_data) %>%
        #notes come in as MST, converting to UTC
        mutate(start_DT = with_tz(start_DT, tzone = "UTC"),
               end_DT = with_tz(end_DT, tzone = "UTC"))

      # Add the field note data to all of the data
      #TODO: check in with FC about their field note taking so we can try to add that in later on

      combined_data <- tidy_data %>%
        future_map(~add_field_notes(df = ., notes = all_field_notes), .progress = TRUE)

      # # Add summary statistics
      summarized_data <- combined_data %>%
        map(~generate_summary_statistics(.))


     return(summarized_data)
    })

  })


  # Filtered data reactive
  filtered_data <- reactive({
    req(loaded_data(), input$date_range, input$sites_select, input$parameters_select)

    start_DT <- as.POSIXct(paste0(input$date_range[1], " 00:01"), tz = "America/Denver")
    end_DT <- as.POSIXct(paste0(input$date_range[2], " 23:55"), tz = "America/Denver")

    sites_sel <- filter(site_table, site_name %in% input$sites_select )%>%
      pull(site_code)

    data <- loaded_data()%>%
      rbindlist()%>%
      mutate(DT_round_MT = with_tz(DT_round, tzone = "America/Denver"))%>%
      filter(between(DT_round_MT, start_DT, end_DT),
             site %in% sites_sel,
             parameter %in% input$parameters_select)


    return(data)
  })

  # Site Map Output
  output$site_map <- renderLeaflet({
    req(values$site_locations)

    pal <- colorFactor(
      palette = c("red", "blue", "green", "purple", "orange"),
      domain = values$site_locations$Project
    )

    leaflet(values$site_locations) %>%
      addTiles() %>%
      addCircleMarkers(
        radius = 8,
        color = ~pal(Project),
        fillOpacity = 0.7,
        popup = ~paste("Site:", Site, "<br>",
                       "Project:", Project)
      ) %>%
      addLegend(
        pal = pal,
        values = ~Project,
        title = "Projects",
        position = "bottomright"
      )
  })

  # Time Series Plot
  output$log_controls <- renderUI({
    data <- filtered_data()
    req(nrow(data) > 0)

    parameters <- unique(data$parameter)

    # Create checkbox inputs for each parameter
    checkbox_list <- lapply(parameters, function(param) {
      # Create a clean input ID from parameter name
      input_id <- paste0("log_", gsub("[^A-Za-z0-9]", "_", tolower(param)))
      checkboxInput(input_id, paste("Log10 Scale -", param), FALSE)
    })

    do.call(tagList, checkbox_list)
  })


  output$dynamic_plots <- renderUI({
    req(input$parameters_select, filtered_data())

    # Calculate number of rows needed (2 plots per row)
    n_params <- length(input$parameters_select)
    n_rows <- ceiling(n_params / 2)

    # Create rows with plots
    plot_rows <- lapply(1:n_rows, function(row) {
      start_idx <- (row - 1) * 2 + 1
      end_idx <- min(row * 2, n_params)

      # Create columns for this row
      cols <- lapply(start_idx:end_idx, function(i) {
        parameter <- input$parameters_select[i]
        plot_id <- paste0("time_series_plot_", i)

        column(
          width = 6,
          h4(paste("Time Series for", parameter)),
          plotlyOutput(plot_id, height = "400px")
        )
      })

      fluidRow(cols)
    })

    do.call(tagList, plot_rows)
  })

  # Generate the actual plots
  observe({
    req(input$parameters_select, filtered_data())

    # Create a plot for each selected parameter
    lapply(seq_along(input$parameters_select), function(i) {
      parameter <- input$parameters_select[i]
      plot_id <- paste0("time_series_plot_", i)

      #browser()
      output[[plot_id]] <- renderPlotly({
        # Filter data for this specific parameter
        plot_data <-  filtered_data() %>%
                      filter(!is.na(mean)) %>%
          filter(parameter == !!parameter)%>%
          left_join(site_table, by = c("site" = "site_code" ))

        # Check if log scale is enabled for this parameter
        log_input_id <- paste0("log_", gsub("[^A-Za-z0-9]", "_", tolower(parameter)))
        use_log <- if (!is.null(input[[log_input_id]])) input[[log_input_id]] else FALSE

        # Create the plotly plot
        p <- plot_ly(plot_data,
                     x = ~DT_round_MT,
                     y = ~mean,
                     type = "scatter",
                     color = ~site,
                     colors = ~color,
                     mode = "lines+markers",
                     name = ~site_name) %>%
          layout(
            xaxis = list(title = "Date"),
            yaxis = list(
              title = if (use_log) paste("Log10(", parameter, ")") else parameter,
              type = if (use_log) "log" else "linear"
            ),
            hovermode = "closest"
          )

        return(p)
      })
    })
  })


#### Flow data Page ####
  # get data for site conditions
  flow_sites_data <- reactive({
    withProgress(message = "Retrieving Poudre flow sites data...", {

      # Retrieve site information using the HUC
      sites <- cdssr::get_telemetry_stations(
        water_district = 3 )%>% # Specify Poudre basin
        filter(station_por_end > Sys.Date() - days(30))%>%# only grab active sites
        filter(grepl("DIS", parameter))#filter for flow sites only

      # If no sites found, return empty tibble
      if (nrow(sites) == 0) {
        return(tibble())
      }

      # Define date range for the last 8 days
      end_date <- as.character(Sys.Date()+ days(1))
      start_date <- as.character(Sys.Date() - days(7))

      # Using purrr to process sites
      active_sites <- sites %>%
        split(1:nrow(.)) %>%
        map_dfr(function(site_row) {
          site_id <- site_row$abbrev
          param_code <- site_row$parameter

          # Try to get recent flow data
          result <- tryCatch({
            flow_data <- cdssr::get_telemetry_ts(
              abbrev = site_id,
              parameter = param_code,
              start_date = start_date,
              end_date = end_date,
              timescale = "raw")%>%
              mutate(DT_round = round_date(datetime, "15 min"))%>%
              group_by(DT_round)%>%
              summarise(flow = mean(meas_value, na.rm = TRUE))%>%
              mutate(abbrev = site_id)

            # Check if we have data
            if (nrow(flow_data) > 1) {
              # Get the most recent 24 hours of data
              end_time <- Sys.time()
              start_time <- end_time - hours(24)

              iv_data <- flow_data %>%
                filter(DT_round >= start_time & DT_round <= end_time)

              if (nrow(iv_data) > 1) {
                # Perform linear regression to calculate slope
                flow_model <- lm(flow ~ DT_round, data = iv_data)
                slope <- coef(flow_model)[2]

                current_flow <- iv_data %>%
                  arrange(DT_round) %>%
                  slice(n()) %>%
                  pull(flow)

                site_row %>%
                  as_tibble() %>%
                  mutate(
                    current_flow_cfs = current_flow,
                    flow_slope = slope * 60 * 60, # Convert slope to cfs/hr
                    trend = if_else(current_flow_cfs == 0, "NoFlow",
                                    if_else(flow_slope > 0, "increasing", "decreasing")),
                    nested_data = list(flow_data)
                  )%>%
                  select(abbrev, station_name, data_source, water_source, gnis_id, latitude, longitude,
                         current_flow_cfs, flow_slope, trend, structure_type, site_type = station_type, nested_data)
              } else {
                tibble()
              }
            } else {
              tibble()
            }
          }, error = function(e) {
            tibble()
          })

          return(result)
        })

      return(active_sites)
    })
  })
  # Create the map of the flow sites
  output$map <- renderLeaflet({


    sites <- flow_sites_data()%>%
      mutate(group_status = case_when(trend == "increasing" & site_type == "Stream Gage" ~ "river_up",
                                      trend == "decreasing" & site_type == "Stream Gage" ~ "river_down",
                                      trend == "increasing" & site_type != "Stream Gage" ~ "ditch_up",
                                      trend == "decreasing" & site_type != "Stream Gage" ~ "ditch_down",
                                      TRUE ~ "no_flow"))
    # Check if sites data is empty
    if (nrow(sites) == 0) {
      return(leaflet() %>%
               addTiles() %>%
               setView(lng = -105.5, lat = 40.6, zoom = 10) %>%
               addControl(html = "No active flow sites found in Cache la Poudre watershed",
                          position = "topright"))
    }

    # Create popup content
    popup_content <- sites %>%
      mutate(
        plot_image = map(nested_data, ~{
          plot <- ggplot(.x, aes(x = DT_round, y = flow)) +
            geom_line(color = "blue") +
            labs(title = "Flow Data", x = "Time", y = "Flow (cfs)") +
            theme_minimal()
          plot_path <- tempfile(fileext = ".png")
          ggsave(plot_path, plot, width = 5, height = 3)
          b64_plot <- base64enc::dataURI(file = plot_path, mime = "image/png")
          file.remove(plot_path)
          paste0('<img src="', b64_plot, '" width="300"/>')
        }),
        popup = str_c(
          "<b>Site Name:</b> ", station_name, "<br>",
          "<b>Site ID:</b> ", abbrev, "<br>",
          "<b>Current Flow:</b> ", round(current_flow_cfs, 1), " cfs<br>",
          "<b>24-hour Trend:</b> ",
          if_else(trend == "increasing",
                  str_c("Increasing (+", round(flow_slope, 1), " cfs/hr)"),
                  str_c("Decreasing (", round(flow_slope, 1), " cfs/hr)")),
          "<br>", plot_image
        )
      ) %>%
      pull(popup)

    # Create labels with current flow
    flow_labels <- sites %>%
      #fix station name to make more sense
      mutate(
        label = if_else(group_status == "no_flow", "", str_c(abbrev, ": ", round(current_flow_cfs, 1), " cfs") %>%
                          htmlEscape())
      ) %>%
      pull(label)

    # Determine icon colors/shape based on trend
    icons <- awesomeIconList(
      river_up = makeAwesomeIcon(icon = "arrow-up", markerColor = "green", library = "fa", squareMarker = F),
      river_down = makeAwesomeIcon(icon = "arrow-down", markerColor = "red", library = "fa", squareMarker = F),
      ditch_up = makeAwesomeIcon(icon = "arrow-up", markerColor = "green", library = "fa", squareMarker = T),
      ditch_down = makeAwesomeIcon(icon = "arrow-down", markerColor = "red", library = "fa", squareMarker = T),
      no_flow = makeAwesomeIcon(icon = "circle", markerColor = "gray", library = "fa")
    )


    leaflet(sites) %>%
      addTiles() %>%
      addAwesomeMarkers(
        lng = ~longitude,
        lat = ~latitude,
        icon = ~icons[group_status],
        group = ~group_status,
        label = lapply(flow_labels, HTML),
        labelOptions = labelOptions(
          noHide = TRUE,
          direction = "auto",
          textSize = "12px",
          style = list(
            "color" = "black",
            "font-weight" = "bold",
            "background-color" = "rgba(255, 255, 255, 0.7)",
            "border" = "1px solid black",
            "padding" = "3px",
            "border-radius" = "3px"
          )
        ),
        popup = popup_content
      ) %>%
      addLegend(
        position = "bottomright",
        colors = c("green", "red", "grey"),
        labels = c("Increasing Flow", "Decreasing Flow", "No Current Flow"),
        title = "24-hour Flow Trend"
      ) %>%
      addProviderTiles("Esri.WorldTopoMap", group = "Topo") %>%
      addProviderTiles("Esri.WorldImagery", group = "Satellite") %>%
      addLayersControl(
        baseGroups = c("Default", "Topo", "Satellite"),
        position = "topright",
        options = layersControlOptions(collapsed = FALSE)
      ) %>%
      setView(lng = -105.5, lat = 40.6, zoom = 10)
  })
  #plot for three days of data
  # Generate 7-day Q plot
  output$three_day_q_plot <- renderPlotly({


    data <- flow_sites_data()%>%
      filter(abbrev %in% c("CLASRKCO", "CLAFTCCO", "CLAFORCO","CLABOXCO","CLARIVCO"))
    #extract the data from nested data
    flow_data <- bind_rows(data$nested_data)%>%
      mutate(site_name = case_when(
        abbrev == "CLASRKCO" ~ "South Fork CLP",
        abbrev == "CLAFTCCO" ~ "CLP @ Canyon Mouth",
        abbrev == "CLAFORCO" ~ "CLP @ Lincoln",
        abbrev == "CLABOXCO" ~ "CLP @ Boxelder",
        abbrev == "CLARIVCO" ~ "CLP @ River Bluffs"
      ))%>%
      filter(DT_round >= Sys.Date() - days(7))

    p <- ggplot(flow_data, aes(x = DT_round, y = flow, color = site_name)) +
      geom_line()+
      scale_color_manual(values = c("South Fork CLP" = "#256BF5",
                                    "CLP @ Canyon Mouth" = "#002EA3",
                                    "CLP @ Lincoln" = "#E70870",
                                    "CLP @ Boxelder" = "#1E4D2B",
                                    "CLP @ River Bluffs" = "#56104E")) +
      labs(x = "Date",
           y = "Discharge (cfs)",
           color = "Site") +
      theme_minimal() +
      theme(axis.text.x = element_text(angle = 45, hjust = 1))

    ggplotly(p)
  })

  # Conditionally show SNOTEL plot between October and July
  output$snotel_card <- renderUI({

    current_month <- month(Sys.Date())

    # Show if month is between October (10) and July (7)
    if(current_month > 10 || current_month < 6) {
      card(
        card_header("Current SNOTEL data for CLP Basin"),
        card_body(
          plotlyOutput("snotel_plot")
        )
      )
    } else {
      # Return empty UI if outside of October-July


      card(
        card_header("Flows Historical Stats"),
        textInput("site_abbrev_selected", "Type site abbreviation from the map below to view historical flow data", value = "CLAFTCCO"),

        card_body(
          plotlyOutput("clp_rainbow_plot")
        )
      )

    }
  })

  # Generate SNOTEL plot (will only be displayed if the card is visible)
  output$snotel_plot <- renderPlotly({


    clp_snotel_url <- "https://nwcc-apps.sc.egov.usda.gov/awdb/basin-plots/POR/WTEQ/assocHUC8/10190007_Cache_La_Poudre.csv"
    #download the csv
    #clp_snotel_data <- read.csv(clp_snotel_url, skip = 1)
    clp_snotel_data <- read.csv(clp_snotel_url)
    #drop the X from names
    names(clp_snotel_data) <- gsub("^X", "", names(clp_snotel_data))

    #grab last year
    cur_year = year(Sys.Date())

    # Reshape the data from wide to long format
    clp_snotel_non_stats <- clp_snotel_data %>%
      select(-c("10.", "30.", "70.", "90.", "Min", "Median...91..20.", "Median..POR.", "Max", "Median.Peak.SWE" ))%>%
      pivot_longer(
        cols = -date,
        names_to = "year",
        values_to = "swe"
      )%>%
      mutate(month_day = date,
             date = ymd(paste0("2000-", month_day)),
             full_date = if_else(
               month(date) >= 10,
               as.Date(paste0(as.numeric(cur_year)-1, "-", month_day), format = "%Y-%m-%d"),
               as.Date(paste0(as.numeric(cur_year), "-", month_day), format = "%Y-%m-%d"))
      )

    curr_year <- clp_snotel_non_stats%>%
      filter(year == as.character(cur_year))%>%
      select(date, swe, full_date)

    cur_swe <- curr_year%>%
      filter(full_date == Sys.Date())%>%
      pull(swe)

    recent_years <- clp_snotel_non_stats%>%
      filter(year <= as.character(cur_year-1) & year >= as.character(cur_year-6))%>%
      select(date, swe, full_date, year)



    # Extract statistical columns
    stats_data <- clp_snotel_data %>%
      select(date, "10.", "30.", "70.", "90.", "Min", "Median...91..20.", "Median..POR.", "Max", "Median.Peak.SWE" )%>%
      pivot_longer(
        cols = -date,
        names_to = "stat",
        values_to = "swe"
      ) %>%
      mutate(stat = case_when(
        stat == "Min" ~ "Min",
        stat == "Median...91..20." ~ "Median ('91-'20)",
        stat == "Median..POR." ~ "Median (POR)",
        stat == "Max" ~ "Max",
        stat == "Median.Peak.SWE" ~ "Median Peak SWE",
        stat == "10." ~ "10th Percentile",
        stat == "30." ~ "30th Percentile",
        stat == "70." ~ "70th Percentile",
        stat == "90." ~ "90th Percentile"))%>%
      filter(!is.na(stat)&!is.na(swe))%>%
      filter(stat != "Median (POR)")%>%
      mutate(month_day = date,
             date = ymd(paste0("2000-", date)),
             full_date = if_else(
               month(date) >= 10,
               as.Date(paste0(as.numeric(cur_year) - 1, "-", month_day), format = "%Y-%m-%d"),
               as.Date(paste0(as.numeric(cur_year), "-", month_day), format = "%Y-%m-%d")
             ))

    percentile_ribbons <- stats_data %>%
      pivot_wider(names_from = stat, values_from = swe)

    # grab the median peak swe stat
    median_peak = clp_snotel_data %>%
      filter(!is.na(`Median.Peak.SWE`))%>%
      select(date, med_peak_swe = `Median.Peak.SWE`)%>%
      mutate(month_day = date,
             date = ymd(paste0("2000-", date)),
             full_date = if_else(
               month(date) >= 10,
               as.Date(paste0(as.numeric(cur_year)-1, "-", month_day), format = "%Y-%m-%d"),
               as.Date(paste0(as.numeric(cur_year) , "-", month_day), format = "%Y-%m-%d")
             ),
             days_until = as.numeric(full_date - Sys.Date()),
             perc_peak = round((cur_swe/med_peak_swe)*100, 0)
      )
    days_until = median_peak%>%
      pull(days_until)

    #grab the median peak swe percent
    perc_peak = median_peak%>%
      filter(full_date == Sys.Date())%>%
      pull(perc_peak)

    #grab the median percent


    perc_med = stats_data%>%
      filter(stat == "Median ('91-'20)" & full_date == Sys.Date())%>%
      mutate(perc_med = round((cur_swe/swe)*100, 0))%>%
      pull(perc_med)


    #create a plotly line plot of each stat over a year
    stats_plot <- plot_ly()%>%
      add_ribbons(
        data = percentile_ribbons,
        x = ~full_date,
        ymin = ~`Min`,
        ymax = ~`10th Percentile`,
        fillcolor = 'rgba(255,0, 0, 0.2)',
        line = list(color = 'transparent'),
        showlegend = FALSE
      )%>%
      add_ribbons(
        data = percentile_ribbons,
        x = ~full_date,
        ymin = ~`10th Percentile`,
        ymax = ~`30th Percentile`,
        fillcolor = 'rgba(255,162, 0, 0.2)',
        line = list(color = 'transparent'),
        showlegend = FALSE
      )%>%
      add_ribbons(
        data = percentile_ribbons,
        x = ~full_date,
        ymin = ~`30th Percentile`,
        ymax = ~`70th Percentile`,
        fillcolor = 'rgba(0,227, 116, 0.2)',
        line = list(color = 'transparent'),
        showlegend = FALSE
      )%>%
      add_ribbons(
        data = percentile_ribbons,
        x = ~full_date,
        ymin = ~`70th Percentile`,
        ymax = ~`90th Percentile`,
        fillcolor = 'rgba(0,255, 252, 0.2)',
        line = list(color = 'transparent'),
        showlegend = FALSE
      )%>%
      add_ribbons(
        data = percentile_ribbons,
        x = ~full_date,
        ymin = ~`90th Percentile`,
        ymax = ~`Max`,
        fillcolor = 'rgba(0,109,255, 0.2)',
        line = list(color = 'transparent'),
        showlegend = FALSE
      )%>%
      add_trace(
        data = stats_data%>%filter(stat %nin% c("10th Percentile", "30th Percentile", "70th Percentile", "90th Percentile")),
        x = ~full_date,
        y = ~swe,
        color =  ~stat,
        type = "scatter",
        mode = "lines",
        line = list(width = 2),
        colors = c("Min" = "rgba(255,0, 0, 0.8)", "Median ('91-'20)" = "rgba(0,227, 116, 0.8)", "Max" = "rgba(0,109,255, 0.8)")) %>%
      add_trace(
        data = median_peak,
        x = ~full_date, y = ~med_peak_swe, type = "scatter", mode = "markers",
        marker = list(color = "green", size = 10, symbol = "x"),
        name = "Median Peak SWE" ) %>%
      add_trace(
        data = curr_year,
        x = ~full_date, y = ~swe, type = "scatter", mode = "lines",
        line = list(color = "black", width = 2),
        name = paste0("Current Year (", cur_year, ")"),
        showlegend = TRUE
      ) %>%
      add_trace(
        data = recent_years,
        x = ~full_date, y = ~swe, type = "scatter", mode = "lines",
        split = ~factor(year, levels = rev(unique(recent_years$year))),
        line = list(width = 0.5),
        showlegend = TRUE,
        name = ~factor(year, levels = rev(unique(recent_years$year))), visible="legendonly"
      ) %>%
      layout(
        title = "SNOTEL SWE Statistics",
        xaxis = list(title = "Date"),
        yaxis = list(title = "Snow Water Equivalent (inches)"),
        showlegend = TRUE) %>%
      layout(
        title = "",
        xaxis = list(title = "",tickformat = "%b %d",range = c(min(stats_data$full_date, na.rm = T),max(stats_data$full_date, na.rm = T))
        ),
        yaxis = list(title = "Snow Water Equivalent (in)", range = c(0, 40)),
        legend = list(x = 1, y = 0.95,title = list(text = "")),
        annotations = list(
          list(
            x = min(stats_data$full_date, na.rm = T) + 15, y = 32.5,
            text = paste0(
              "Current as of ", Sys.Date(), ":<br>",
              "Current Basin SWE:", cur_swe, "<br>",
              "% of Median:", perc_med , " <br>",
              "% Median Peak:", pull(median_peak, name = perc_peak), " <br>",
              "Days Until Median Peak:",days_until," <br>"
            ),
            showarrow = FALSE,bordercolor = "black",borderwidth = 1,bgcolor = "white",xanchor = "left"
          )
        ),
        margin = list(t = 40, r = 100, b = 40, l = 60)
      )

    stats_plot

  })


  output$clp_rainbow_plot <- renderPlotly({
    source("src/create_rainbow_flow_plot.R")

    plot_title <- cdssr::get_sw_stations(abbrev = input$site_abbrev_selected)%>%
      pull(station_name)%>%
      str_to_title()


    create_rainbow_flow_plot(station_abbrev = input$site_abbrev_selected,
                                                     station_plot_name = plot_title,
                                                      years = 30,
                                                      incl_winter = F)

  })



}

# Run the application
shinyApp(ui = ui, server = server)
